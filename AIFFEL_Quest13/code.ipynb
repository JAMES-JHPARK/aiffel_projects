{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "1. 챗봇 훈련데이터 전처리 과정이 체곚거으로 진행되었는가?  \n",
    "   챗봇 훈련데이터를 위한 전처리와 Augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.  \n",
    "2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?  \n",
    "   과적합을 피할 . . 숭ㅆ는 하이퍼파라미터 셋이 적절히 제시되었다.  \n",
    "3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?  \n",
    "   주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드\n",
    "ChabotData.csv를 pandas를 이용해 읽어온 후, 데이터의 질문과 답변을 각각 question, answers 변수에 나눠서 저장.  \n",
    "[songys/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/ChatbotData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df['Q']\n",
    "answers = df['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제\n",
    "preprocess_sentence() 함수 구현  \n",
    "- 영문자의 경우, 모두 소문자로 변환\n",
    "- 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거  \n",
    "\n",
    "이전과 다르게 생략된 기능들은 토크나이저에서 지원."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"[^0-9ㄱ-ㅣ가-힣a-zA-Z?.!]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화\n",
    "KoNLPy의 mecab 클래스 사용.  \n",
    "build_corpus() 함수 구현\n",
    "- 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받음.\n",
    "- 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화\n",
    "- 토큰화는 전달받은 토크나이즈 함수를 사용. mecab.morphs 함수를 전달\n",
    "- 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외\n",
    "- 중복되는 문장은 데이터에서 제외. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사. 중복 쌍이 흐트러지지 않도록 유의.  \n",
    "\n",
    "구현한 함수를 활용하여 qeustions와 answers를 각각 que_corpus, ans_corpus에 토큰화하여 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src_sentences, tgt_sentences, tokenize_fn, max_len = 40):\n",
    "    src_corpus = []\n",
    "    tgt_corpus = []\n",
    "    src_seen = set()\n",
    "    tgt_seen = set()\n",
    "\n",
    "    for src, tgt in zip(src_sentences, tgt_sentences):\n",
    "        src = preprocess_sentence(src)\n",
    "        tgt = preprocess_sentence(tgt)\n",
    "\n",
    "        src_tokens = tokenize_fn(src)\n",
    "        tgt_tokens = tokenize_fn(tgt)\n",
    "\n",
    "        if len(src_tokens) <= max_len and len(tgt_tokens) <= max_len:\n",
    "            src_joined = ' '.join(src_tokens)\n",
    "            tgt_joined = ' '.join(tgt_tokens)\n",
    "            if src_joined not in src_seen and tgt_joined not in tgt_seen:\n",
    "                src_corpus.append(src_tokens)\n",
    "                tgt_corpus.append(tgt_tokens)\n",
    "                src_seen.add(src_joined)\n",
    "                tgt_seen.add(tgt_joined)\n",
    "\n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7575 7575\n",
      "\n",
      "que_corpus: \n",
      "12 시 땡 !\n",
      "1 지망 학교 떨어졌 어\n",
      "3 박 4 일 놀 러 가 고 싶 다\n",
      "ppl 심하 네\n",
      "sd 카드 망가졌 어\n",
      "\n",
      "ans_corpus: \n",
      "하루 가 또 가 네요 .\n",
      "위로 해 드립니다 .\n",
      "여행 은 언제나 좋 죠 .\n",
      "눈살 이 찌푸려 지 죠 .\n",
      "다시 새로 사 는 게 마음 편해요 .\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers, mecab.morphs, max_len = 20)\n",
    "\n",
    "print(len(que_corpus), len(ans_corpus))\n",
    "print()\n",
    "print('que_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in que_corpus[:5]]), sep = '\\n')\n",
    "print()\n",
    "print('ans_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in ans_corpus[:5]]), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Augmentation\n",
    "Lexical Substitution을 실제로 적용.  \n",
    "아래 링크를 참고하여 한국어로 사전 훈련된 Embedding 모델을 다운로드. Korean (w)가 Word2Vec으로 학습한 모델이며 용량도 적당. Korean (w) 다운로드. ko.bin 파일 얻기.  \n",
    "[Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)  \n",
    "다운로드한 모델을 활용해 데이터 Augmentation. lexical_sub() 함수 참고.  \n",
    "Augmentation된 que_corpus와 원본 ans_corpus가 병렬을 이루도록, 원본 que_corpus와 Augmentation된 ans_corpus가 병렬을 이루도록 하여 전체 데이터가 원래의 3배 가량으로 늘어나도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv = gensim.models.Word2Vec.load('./data/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv = gensim.models.KeyedVectors.load_word2vec_format('./data/ko.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = Word2VecKeyedVectors.load('./data/word2vec_ko.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(tokens):\n",
    "    selected_tok = random.choice(tokens)\n",
    "\n",
    "    try:\n",
    "        similar_word = word_vectors.wv.similar_by_word(selected_tok)[0][0]\n",
    "    except KeyError:\n",
    "        similar_word = selected_tok\n",
    "        # print('not changed', 'tokens:', tokens)\n",
    "    \n",
    "    return [similar_word if tok == selected_tok else tok for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12', '시경', '땡', '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = que_corpus[0]\n",
    "\n",
    "selected_tok = random.choice(tokens)\n",
    "\n",
    "try:\n",
    "    similar_word = word_vectors.wv.similar_by_word(selected_tok)[0][0]\n",
    "except KeyError:\n",
    "    similar_word = selected_tok\n",
    "    # print('not changed', 'tokens:', tokens)\n",
    "\n",
    "[similar_word if tok == selected_tok else tok for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_data(que_corpus, ans_corpus):\n",
    "    augmented_que_corpus = []\n",
    "    augmented_ans_corpus = []\n",
    "\n",
    "    augmented_que_corpus.extend(que_corpus)\n",
    "    augmented_ans_corpus.extend(ans_corpus)\n",
    "\n",
    "    for question in que_corpus:\n",
    "        augmented_question = lexical_sub(question)\n",
    "        augmented_que_corpus.append(augmented_question)\n",
    "        augmented_ans_corpus.append(ans_corpus[que_corpus.index(question)])\n",
    "    \n",
    "    for answer in ans_corpus:\n",
    "        augmented_answer = lexical_sub(answer)\n",
    "        augmented_ans_corpus.append(augmented_answer)\n",
    "        augmented_que_corpus.append(que_corpus[ans_corpus.index(answer)])\n",
    "    \n",
    "    return augmented_que_corpus, augmented_ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22725 22725\n",
      "\n",
      "que_corpus: \n",
      "12 시 땡 !\n",
      "1 지망 학교 떨어졌 어\n",
      "3 박 4 일 놀 러 가 고 싶 다\n",
      "ppl 심하 네\n",
      "sd 카드 망가졌 어\n",
      "\n",
      "ans_corpus: \n",
      "하루 가 또 가 네요 .\n",
      "위로 해 드립니다 .\n",
      "여행 은 언제나 좋 죠 .\n",
      "눈살 이 찌푸려 지 죠 .\n",
      "다시 새로 사 는 게 마음 편해요 .\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = augmentation_data(que_corpus, ans_corpus)\n",
    "\n",
    "print(len(que_corpus), len(ans_corpus))\n",
    "print()\n",
    "print('que_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in que_corpus[:5]]), sep = '\\n')\n",
    "print()\n",
    "print('ans_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in ans_corpus[:5]]), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicate(que_corpus, ans_corpus):\n",
    "    que_tokens = []\n",
    "    ans_tokens = []\n",
    "\n",
    "    que_seen = set()\n",
    "    ans_seen = set()\n",
    "\n",
    "    for i in range(len(que_corpus)):\n",
    "        que_joined = ' '.join(que_corpus[i])\n",
    "        ans_joined = ' '.join(ans_corpus[i])\n",
    "\n",
    "        if que_joined in que_seen and ans_joined in ans_seen:\n",
    "            pass\n",
    "        else:\n",
    "            que_tokens.append(que_corpus[i])\n",
    "            ans_tokens.append(ans_corpus[i])\n",
    "            que_seen.add(que_joined)\n",
    "            ans_seen.add(ans_joined)\n",
    "    \n",
    "    return que_tokens, ans_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22508 22508\n",
      "\n",
      "que_corpus: \n",
      "12 시 땡 !\n",
      "1 지망 학교 떨어졌 어\n",
      "3 박 4 일 놀 러 가 고 싶 다\n",
      "ppl 심하 네\n",
      "sd 카드 망가졌 어\n",
      "\n",
      "ans_corpus: \n",
      "하루 가 또 가 네요 .\n",
      "위로 해 드립니다 .\n",
      "여행 은 언제나 좋 죠 .\n",
      "눈살 이 찌푸려 지 죠 .\n",
      "다시 새로 사 는 게 마음 편해요 .\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = delete_duplicate(que_corpus, ans_corpus)\n",
    "\n",
    "print(len(que_corpus), len(ans_corpus))\n",
    "print()\n",
    "print('que_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in que_corpus[:5]]), sep = '\\n')\n",
    "print()\n",
    "print('ans_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in ans_corpus[:5]]), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. 데이터 벡터화\n",
    "ans_corpus에 \\<start\\> 토큰과 \\<end\\> 토큰을 추가 후 벡터화 진행.  \n",
    "```python\n",
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])\n",
    "```\n",
    "챗봇 훈련 데이터는 소스 데이터와 타겟 데이터가 같은 언어를 사용. Embedding 층을 공유했을 때 많은 이점.  \n",
    "- ans_corpus와 que_corpus를 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train과 dec_train을 얻기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. 훈련하기\n",
    "앞서 번역 모델을 훈련하며 정의한 Transformer를 그대로 사용.  \n",
    "데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있음.  \n",
    "모델을 훈련하고 아래 예문에 대한 답변을 생성.\n",
    "```python\n",
    "# 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "```\n",
    "---\n",
    "```python\n",
    "# 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. 성능 측정하기\n",
    "주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
